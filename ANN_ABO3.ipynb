{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automatic-corner",
   "metadata": {
    "id": "iraqi-thriller"
   },
   "source": [
    "## Introduction\n",
    "This notebook aims to replicate the studies that were successful in predicting the lattice constants of the Perovskite crystal ($ABO_{3}$). The papers preoduced the results with the model's error around 2%. It's important to identify the features neccessary to perform predictions. The lattice constants of the $ABO_{3}$ -type may be correlated compounds as a general function of nine variables as below\n",
    "\n",
    "$Lattice constant = f(z_A + z_B + z_O + r_A + r_B + r_O + x_A + x_B + x_O)$\n",
    "\n",
    "where z, r and x denote the valence, the radius and the electronegativity of the ions (A, B, O), respectively. It is noted that the three variables associated with anion $O^{2-}$ , namely $z_{O}, r_{O},$ and $x_{O}$ can be ignored as they remain unchanged for all of the samples.\n",
    "\n",
    "The lattice constant can be reduced as a ﬁve-parameter function shown below.\n",
    "\n",
    "$Lattice constant = f(z_A + r_A + r_B + x_A + x_B)$\n",
    "\n",
    "The Goldschmidt’s tolerance factor (t) can condense the coation atomic radii into one expression. It has been widely accepted as a criterion for the formation of the perovskite structure, up to now. I will use $\\textbf{four features}$ including the tolerance factor and others are denoted by, $r_{A}, r_{B}$ and $\\frac{r_{A}}{t}$. The list of feature does not include electronegativity and valence number. Basically, other sets of features that includes these two atomic configurations can be used to predict the lattice constant with great accuracy. However, I will only take the atomic radii of the two cations in to my account. The tolerance factor (t) is expressed as below.\n",
    "\n",
    "$t = \\frac{r_A + r_O}{\\sqrt{2}(r_B + r_O)}$\n",
    "\n",
    "## Procedure\n",
    "First I will perform feature engineering. Next I will build the ANN (Hereafter called as ANN) model and also perform a GridSearch (GS). GS fine tunes the hyperparameters of the ANN. Finally I will perform the predictions from the model. The model will be prepared with the fine tuned hyperparameters. The GridSearch in the sklearn package convineintly provides the best model which can be used to fit and predict.\n",
    "\n",
    "## Feature engineering\n",
    "In the following section, the \".csv\" file is loaded into the program. The data is inspected for assurity. I observed the '.csv' provides the atomic radii for two atoms A and B in the crystal ($r_{A}$ and $r_{b}$), and the tolerance factor is absent. So the tolerance factor and $\\frac{r_{A}}{t}$ needs to be calculated. I have concanated them as columns in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lesser-strike",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "actual-cemetery",
    "outputId": "6dbe1700-fcb6-4d60-b8f7-e8a6b217be5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five smaples from the DataFrame\n",
      "\n",
      "            ra    rb    xa    xb  za       a       b       c\n",
      "compound                                                    \n",
      "BaCeO3    1.35  0.87  0.89  1.12   2  6.2350  8.7810  6.2120\n",
      "BaPrO3    1.35  0.85  0.89  1.13   2  6.2140  8.7220  6.1810\n",
      "BaPuO3    1.35  0.86  0.89  1.30   2  6.1930  8.7440  6.2190\n",
      "CaCrO3    1.00  0.55  1.00  1.66   2  5.3160  7.4860  5.2870\n",
      "CaGeO3    1.00  0.53  1.00  2.01   2  5.2688  7.4452  5.2607\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "lat_df = pd.read_csv('lat_const.csv', header=0, index_col = 0, usecols= ['compound','ra','rb','xa','xb','za','a','b','c'])\n",
    "print (\"The first five smaples from the DataFrame\\n\")\n",
    "print (lat_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dietary-actor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "editorial-provider",
    "outputId": "759b4126-9bf4-4762-c1a1-dc53a770cdd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five rows of imputed DataFrame\n",
      "\n",
      "            ra    rb    xa    xb  za       a       b       c  tolerance  \\\n",
      "compound                                                                  \n",
      "BaCeO3    1.35  0.87  0.89  1.12   2  6.2350  8.7810  6.2120   0.856627   \n",
      "BaPrO3    1.35  0.85  0.89  1.13   2  6.2140  8.7220  6.1810   0.864242   \n",
      "BaPuO3    1.35  0.86  0.89  1.30   2  6.1930  8.7440  6.2190   0.860418   \n",
      "CaCrO3    1.00  0.55  1.00  1.66   2  5.3160  7.4860  5.2870   0.870285   \n",
      "CaGeO3    1.00  0.53  1.00  2.01   2  5.2688  7.4452  5.2607   0.879304   \n",
      "\n",
      "               rat  \n",
      "compound            \n",
      "BaCeO3    1.575948  \n",
      "BaPrO3    1.562063  \n",
      "BaPuO3    1.569006  \n",
      "CaCrO3    1.149049  \n",
      "CaGeO3    1.137263  \n",
      "\n",
      "The first five observations of the features\n",
      "[[1.35       0.87       0.85662717 1.5759481 ]\n",
      " [1.35       0.85       0.8642416  1.5620632 ]\n",
      " [1.35       0.86       0.86041754 1.5690056 ]\n",
      " [1.         0.55       0.8702853  1.1490486 ]\n",
      " [1.         0.53       0.87930375 1.1372634 ]]\n",
      "\n",
      "The first five observations of the targets\n",
      "[[6.235  8.781  6.212 ]\n",
      " [6.214  8.722  6.181 ]\n",
      " [6.193  8.744  6.219 ]\n",
      " [5.316  7.486  5.287 ]\n",
      " [5.2688 7.4452 5.2607]]\n"
     ]
    }
   ],
   "source": [
    "lat_df = lat_df.assign(tolerance = lambda x:((x['ra'] + 1.4)/(np.sqrt(2)*(x['rb'] + 1.4))))\n",
    "lat_df = lat_df.assign(rat = lambda x:(x['ra']/x['tolerance']))\n",
    "\n",
    "print (\"The first five rows of imputed DataFrame\\n\")\n",
    "print (lat_df.head())\n",
    "\n",
    "features = np.array(lat_df[['ra', 'rb', 'tolerance', 'rat']], np.float32)\n",
    "targets = np.array(lat_df[['a', 'b', 'c']], np.float32)\n",
    "\n",
    "print (\"\\nThe first five observations of the features\")\n",
    "print (features[:5,:])\n",
    "print (\"\\nThe first five observations of the targets\")\n",
    "print (targets[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-combat",
   "metadata": {},
   "source": [
    "## Machine Learning Model\n",
    "\n",
    "The prediction of lattice constants for $ABO_{3}$ from the atomic configurations like radii and tolerance has enigmatic mechanism. A linear regression model can not be conceived to solve multiple target variables. It can be deployed only in the case of single target variable. Here, the artificial neural network comes to yhe rescue. ANNs performance is remarkable for regression model. ANN works for any kind of numerical system.\n",
    "\n",
    "For the ANN, I will choose the loss function as the mean absolute error. \"MAE\" does not penalise the outliers in predictions compared to mean square error and huber loss function. I will choose Adam optimizer over Stochastic gradient descent (SGD). Besides Adam optimizer the other hyperparameters are number of neural nodes, number of hidden layers, learning rate (Adam optimizer), activation functions and so on. The ANN has lots of hyperparameters to be tuned before I set out to predict the target variables. So a GS will be neccessary to narrow down the optimal set of hyperparameters.\n",
    "\n",
    "A quick run down to my model's parameter is below.\n",
    "1. Number of epochs = 500\n",
    "2. Train and test data split ratio = 80:20\n",
    "3. Number of hidden layers = 2\n",
    "4. Number of neurons in each hidden layer = 400\n",
    "\n",
    "Once the model finishes its job, the loss (mean absolute error) and the graph for train and test loss against number of epochs is shown. Next, I will perform the GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "square-sunglasses",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "weird-offense",
    "outputId": "21326357-b249-4129-816b-cfd4692d28d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 6.0743 - val_loss: 5.7093\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 5.5759 - val_loss: 5.2067\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 5.0445 - val_loss: 4.5794\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 4.3772 - val_loss: 3.7700\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 3.5077 - val_loss: 2.7160\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.3769 - val_loss: 1.3612\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0317 - val_loss: 0.8424\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8600 - val_loss: 1.2163\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2176 - val_loss: 1.0472\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.8567 - val_loss: 0.3156\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3313 - val_loss: 0.6057\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6492 - val_loss: 0.6327\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5450 - val_loss: 0.3239\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2947 - val_loss: 0.4239\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3920 - val_loss: 0.3046\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2432 - val_loss: 0.3485\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3401 - val_loss: 0.3299\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2620 - val_loss: 0.2753\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2626 - val_loss: 0.2887\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2308 - val_loss: 0.2678\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2387 - val_loss: 0.2555\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2161 - val_loss: 0.2649\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2152 - val_loss: 0.2571\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2208 - val_loss: 0.2650\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2196 - val_loss: 0.2382\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1990 - val_loss: 0.2359\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1927 - val_loss: 0.2354\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1898 - val_loss: 0.2338\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1923 - val_loss: 0.2269\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1873 - val_loss: 0.2248\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1831 - val_loss: 0.2204\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1821 - val_loss: 0.2176\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1815 - val_loss: 0.2156\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1793 - val_loss: 0.2135\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1768 - val_loss: 0.2122\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1745 - val_loss: 0.2053\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1848 - val_loss: 0.2053\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1767 - val_loss: 0.2064\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1720 - val_loss: 0.2094\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1692 - val_loss: 0.2060\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1753 - val_loss: 0.1954\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1721 - val_loss: 0.2000\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1868 - val_loss: 0.2028\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1720 - val_loss: 0.2011\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1747 - val_loss: 0.1938\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1625 - val_loss: 0.1915\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1645 - val_loss: 0.1862\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1575 - val_loss: 0.1763\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1521 - val_loss: 0.1725\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1507 - val_loss: 0.1676\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1443 - val_loss: 0.1622\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1438 - val_loss: 0.1622\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1392 - val_loss: 0.1578\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1350 - val_loss: 0.1615\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1431 - val_loss: 0.1575\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1353 - val_loss: 0.1504\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1353 - val_loss: 0.1598\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1465 - val_loss: 0.1525\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1391 - val_loss: 0.1470\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1312 - val_loss: 0.1450\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1339 - val_loss: 0.1438\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1279 - val_loss: 0.1350\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1204 - val_loss: 0.1301\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1154 - val_loss: 0.1338\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1242 - val_loss: 0.1310\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1228 - val_loss: 0.1209\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1083 - val_loss: 0.1128\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1025 - val_loss: 0.1180\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1091 - val_loss: 0.1271\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1096 - val_loss: 0.1347\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1216 - val_loss: 0.1097\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1033 - val_loss: 0.1213\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1088 - val_loss: 0.1173\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1050 - val_loss: 0.1253\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1074 - val_loss: 0.1014\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0967 - val_loss: 0.0919\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0900 - val_loss: 0.0893\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0958 - val_loss: 0.1198\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1155 - val_loss: 0.1223\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1104 - val_loss: 0.1146\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1098 - val_loss: 0.0893\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1037 - val_loss: 0.0786\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0886 - val_loss: 0.0769\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0828 - val_loss: 0.0795\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0799 - val_loss: 0.0794\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0805 - val_loss: 0.0758\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0804 - val_loss: 0.0796\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0862 - val_loss: 0.0756\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0847 - val_loss: 0.0914\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0856 - val_loss: 0.0818\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0868 - val_loss: 0.0803\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0839 - val_loss: 0.0672\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0734 - val_loss: 0.0641\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0682 - val_loss: 0.0655\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0697 - val_loss: 0.0612\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0661 - val_loss: 0.0634\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0703 - val_loss: 0.0633\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0696 - val_loss: 0.0756\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0744 - val_loss: 0.0691\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0697 - val_loss: 0.0657\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0761 - val_loss: 0.0660\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0699 - val_loss: 0.0630\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0689 - val_loss: 0.0593\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0653 - val_loss: 0.0576\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0647 - val_loss: 0.0614\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0670 - val_loss: 0.0641\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0711 - val_loss: 0.0583\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0652 - val_loss: 0.0689\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0655 - val_loss: 0.0575\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0645 - val_loss: 0.0637\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0666 - val_loss: 0.0552\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0617 - val_loss: 0.0608\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0627 - val_loss: 0.0575\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0628 - val_loss: 0.0683\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0736 - val_loss: 0.0604\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0662 - val_loss: 0.0609\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0669 - val_loss: 0.0583\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0659 - val_loss: 0.0606\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0652 - val_loss: 0.0564\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0629 - val_loss: 0.0586\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0688 - val_loss: 0.0560\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0625 - val_loss: 0.0575\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0615 - val_loss: 0.0631\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0656 - val_loss: 0.0591\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0647 - val_loss: 0.0579\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0668 - val_loss: 0.0652\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0693 - val_loss: 0.0779\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0729 - val_loss: 0.0920\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0988 - val_loss: 0.0774\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0806 - val_loss: 0.0755\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0725 - val_loss: 0.0745\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0824 - val_loss: 0.0655\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0661 - val_loss: 0.0700\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0676 - val_loss: 0.0627\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0667 - val_loss: 0.0593\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0737 - val_loss: 0.0706\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0826 - val_loss: 0.0698\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0689 - val_loss: 0.0573\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0691 - val_loss: 0.0635\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0654 - val_loss: 0.0567\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0627 - val_loss: 0.0597\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0651 - val_loss: 0.0634\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0668 - val_loss: 0.0651\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0680 - val_loss: 0.0689\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0674 - val_loss: 0.0628\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0675 - val_loss: 0.0859\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0806 - val_loss: 0.0641\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0766 - val_loss: 0.0664\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0797 - val_loss: 0.0759\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0723 - val_loss: 0.0618\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0706 - val_loss: 0.0573\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0573\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The predicted values (Lattice constants)\n",
      "[[5.75807   8.121058  5.778562 ]\n",
      " [5.343145  7.4008    5.234465 ]\n",
      " [5.617514  7.749297  5.468176 ]\n",
      " [5.5376353 7.698663  5.4503636]\n",
      " [5.5500712 7.5574307 5.305666 ]\n",
      " [5.631352  7.59602   5.3109584]\n",
      " [5.613946  7.7964764 5.516129 ]\n",
      " [5.536228  7.7226114 5.47452  ]\n",
      " [5.6276784 7.6441693 5.360084 ]\n",
      " [5.538552  7.6868796 5.438366 ]\n",
      " [5.628648  7.6322174 5.3478675]\n",
      " [5.5525413 7.520991  5.268612 ]\n",
      " [5.626676  7.656064  5.3722577]\n",
      " [5.76767   7.263147  4.9216003]\n",
      " [5.9421186 8.299526  5.8802752]\n",
      " [5.626676  7.656064  5.3722577]\n",
      " [5.617514  7.749297  5.468176 ]\n",
      " [6.1520314 8.707996  6.204751 ]\n",
      " [5.3900743 7.495974  5.3096137]\n",
      " [5.6662726 7.8463974 5.544336 ]]\n",
      "\n",
      " The mean absolute error (MAE)\n",
      "-0.05727582424879074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The seed is required for replication of results. Parameters defined.\n",
    "seed = 1111\n",
    "tolerance_value = 40\n",
    "n_epochs = 500\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets,\n",
    "                                                    random_state=seed,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "def build_model(n_hidden=2, n_neurons=400, learning_rate=1e-3, input_shape=[4,]):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(3))\n",
    "\n",
    "    model.compile(loss=\"mae\", optimizer=keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    return model\n",
    "\n",
    "# The wrapper is neccessary for GridSearch later in this notebook\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "# The history dictionary is useful for 'loss' plot of the model. In this case, the 'loss' is mean absolute error \n",
    "history = keras_reg.fit(X_train, y_train, epochs=n_epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=tolerance_value)])\n",
    "\n",
    "# Test on the holdout data\n",
    "mae_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print (\"\\n The predicted values (Lattice constants)\")\n",
    "print (y_pred)\n",
    "print (\"\\n The mean absolute error (MAE)\")\n",
    "print (mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-complex",
   "metadata": {
    "id": "atlantic-arrow"
   },
   "source": [
    "## GridSearch\n",
    "Grid search (GS) as the name suggests requires a grid made up by the hyperparamters. The alogorithm then runs the model over the grid and finds the best set of parameters for which the model perform best, i.e. low error (MSE, MAE, huber loss, etc in case of refression). The algorithm is heavy because it is performing a brute force approach. There are some techniques like Bayesian inference methods which are very quick in findig the hyperparameters. \n",
    "\n",
    "In the GS, I will set cross-validation option which results in more accurate loss metrics. Cross-validation is a statistical method of evaluating generalization performance that is more stable and thorough than using a split into a training and a test set.\n",
    "\n",
    "The parameter grid for my models are bulleted below.\n",
    "\n",
    "1.   number of hidden layers (n_hidden)\n",
    "2.   number of neural nodes in each hidden layers (n_neurons)\n",
    "3.   learning rate for Adam optimizer\n",
    "\n",
    "Since GS is a type of brute force approach I have employed a callback function which will terminate the GS once the patience level is crossed. This way I do not have to lose the computational power once the model loss starts saturating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "concerned-publication",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y2FNniocgLzQ",
    "outputId": "46dbc949-688d-4577-c759-1fedbe6a267b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 267ms/step - loss: 4.6477 - val_loss: 1.4926\n",
      "Best: -1.271604 using {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 2800}\n",
      "mean: -2.819318, std: (0.115264) parameters: {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 2800}\n",
      "mean: -2.451722, std: (0.193781) parameters: {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 3000}\n",
      "mean: -2.111827, std: (0.094086) parameters: {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 3200}\n",
      "mean: -1.595243, std: (0.263243) parameters: {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 3400}\n",
      "mean: -1.271604, std: (0.082684) parameters: {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 2800}\n",
      "mean: -1.439196, std: (0.299803) parameters: {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 3000}\n",
      "mean: -2.668673, std: (0.162618) parameters: {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 3200}\n",
      "mean: -3.416515, std: (0.293726) parameters: {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 3400}\n",
      "mean: -6.350949, std: (0.769336) parameters: {'learning_rate': 0.001, 'n_hidden': 5, 'n_neurons': 2800}\n",
      "mean: -7.001850, std: (0.764920) parameters: {'learning_rate': 0.001, 'n_hidden': 5, 'n_neurons': 3000}\n",
      "mean: -11.048165, std: (0.969225) parameters: {'learning_rate': 0.001, 'n_hidden': 5, 'n_neurons': 3200}\n",
      "mean: -13.118546, std: (1.154542) parameters: {'learning_rate': 0.001, 'n_hidden': 5, 'n_neurons': 3400}\n",
      "mean: -6.003836, std: (0.008842) parameters: {'learning_rate': 0.0001, 'n_hidden': 3, 'n_neurons': 2800}\n",
      "mean: -6.003040, std: (0.017463) parameters: {'learning_rate': 0.0001, 'n_hidden': 3, 'n_neurons': 3000}\n",
      "mean: -5.970785, std: (0.014513) parameters: {'learning_rate': 0.0001, 'n_hidden': 3, 'n_neurons': 3200}\n",
      "mean: -5.941090, std: (0.008714) parameters: {'learning_rate': 0.0001, 'n_hidden': 3, 'n_neurons': 3400}\n",
      "mean: -6.024580, std: (0.014325) parameters: {'learning_rate': 0.0001, 'n_hidden': 4, 'n_neurons': 2800}\n",
      "mean: -5.995741, std: (0.019109) parameters: {'learning_rate': 0.0001, 'n_hidden': 4, 'n_neurons': 3000}\n",
      "mean: -5.992954, std: (0.009774) parameters: {'learning_rate': 0.0001, 'n_hidden': 4, 'n_neurons': 3200}\n",
      "mean: -5.976624, std: (0.019985) parameters: {'learning_rate': 0.0001, 'n_hidden': 4, 'n_neurons': 3400}\n",
      "mean: -6.038765, std: (0.014903) parameters: {'learning_rate': 0.0001, 'n_hidden': 5, 'n_neurons': 2800}\n",
      "mean: -6.039963, std: (0.005789) parameters: {'learning_rate': 0.0001, 'n_hidden': 5, 'n_neurons': 3000}\n",
      "mean: -6.008291, std: (0.023073) parameters: {'learning_rate': 0.0001, 'n_hidden': 5, 'n_neurons': 3200}\n",
      "mean: -6.003939, std: (0.015294) parameters: {'learning_rate': 0.0001, 'n_hidden': 5, 'n_neurons': 3400}\n"
     ]
    }
   ],
   "source": [
    "# GridSearch for the ANN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_distribs = {\n",
    "#     \"n_hidden\": [1, 2, 3, 4],\n",
    "#     \"n_neurons\": [900, 1100, 1300, 1500],\n",
    "#     \"learning_rate\": [1e-2, 1e-3],\n",
    "# }\n",
    "\n",
    "tolerance_value = 80\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [3, 4, 5],\n",
    "    \"n_neurons\": [2700, 2800, 2900, 3000, 3100],\n",
    "    \"learning_rate\": [1e-3, 1e-4],\n",
    "}\n",
    "\n",
    "search_cv = GridSearchCV(estimator=keras_reg,\n",
    "                         param_grid=param_distribs, \n",
    "                         cv=4, \n",
    "                         scoring ='neg_mean_absolute_error',\n",
    "                        #  refit=False,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "\n",
    "# Best estimator is selected for further proceedings\n",
    "grid_result = search_cv.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=tolerance_value)])\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"mean: %f, std: (%f) parameters: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-arizona",
   "metadata": {},
   "source": [
    "The result from GS is in. And the quick look can be done in following bullets\n",
    "- number of hidden layers (n_hidden) =\n",
    "- number of neural nodes in each hidden layers (n_neurons) =\n",
    "- learning rate for Adam optimizer =\n",
    "\n",
    "I will now extract the best performing model from the GS object. This model is already set up with the set of hyperparameters for which the loss was minimium. The outcome of the model is same as before and is appended to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "transsexual-twenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " History objects\n",
      "Epoch 1/500\n",
      "3/3 [==============================] - 1s 150ms/step - loss: 4.7190 - val_loss: 1.1576\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 1.4460 - val_loss: 1.0532\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 1.0931 - val_loss: 0.5019\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.6304 - val_loss: 1.2420\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.9616 - val_loss: 1.5608\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 1.1611 - val_loss: 1.5366\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 1.0379 - val_loss: 1.2873\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.9452 - val_loss: 1.2429\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.8882 - val_loss: 1.2225\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.9602 - val_loss: 0.9466\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.7531 - val_loss: 0.7373\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.5546 - val_loss: 0.5359\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.5501 - val_loss: 0.6479\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.5758 - val_loss: 0.3322\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.4507 - val_loss: 0.2833\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.5141 - val_loss: 0.6168\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.7018 - val_loss: 0.5811\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.4849 - val_loss: 0.5239\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.6106 - val_loss: 0.4955\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.4830 - val_loss: 0.5023\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.4284 - val_loss: 0.4854\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.4452 - val_loss: 0.5272\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.4309 - val_loss: 0.3240\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.4626 - val_loss: 0.3919\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.3412 - val_loss: 0.1964\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.3683 - val_loss: 0.3442\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3300 - val_loss: 0.2135\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.4374 - val_loss: 0.2842\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.5903 - val_loss: 0.3005\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.3284 - val_loss: 0.3004\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.2523 - val_loss: 0.1579\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.1687 - val_loss: 0.4332\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.3405 - val_loss: 0.6319\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.4636 - val_loss: 0.4788\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.4618 - val_loss: 0.1918\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2465 - val_loss: 0.1017\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 2s 1s/step - loss: 0.1750 - val_loss: 0.0979\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2477 - val_loss: 0.5178\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.4392 - val_loss: 0.2930\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.3827 - val_loss: 0.2139\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.2629 - val_loss: 0.3124\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.2667 - val_loss: 0.1663\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.2539 - val_loss: 0.2811\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2181 - val_loss: 0.2970\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2651 - val_loss: 0.2377\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.3092 - val_loss: 0.3182\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.2830 - val_loss: 0.2210\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2372 - val_loss: 0.2160\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2225 - val_loss: 0.1514\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1829 - val_loss: 0.1092\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.1499 - val_loss: 0.1756\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.2214 - val_loss: 0.4400\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.3387 - val_loss: 0.2935\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.2667 - val_loss: 0.1748\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2896 - val_loss: 0.1922\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2289 - val_loss: 0.2186\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2200 - val_loss: 0.0837\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.1363 - val_loss: 0.1858\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.1656 - val_loss: 0.1085\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.1324 - val_loss: 0.2074\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.2173 - val_loss: 0.1393\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.1788 - val_loss: 0.2391\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2551 - val_loss: 0.4005\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.3752 - val_loss: 0.3209\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.2977 - val_loss: 0.3259\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.2689 - val_loss: 0.1383\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2215 - val_loss: 0.2022\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.1825 - val_loss: 0.1007\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.1876 - val_loss: 0.1850\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.3811 - val_loss: 0.3309\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.3229 - val_loss: 0.1179\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.3116 - val_loss: 0.2968\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2824 - val_loss: 0.4588\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3444 - val_loss: 0.4004\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.3790 - val_loss: 0.2374\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1983 - val_loss: 0.3025\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2859 - val_loss: 0.1988\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.2470 - val_loss: 0.5313\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.4648 - val_loss: 0.5791\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.4821 - val_loss: 0.4615\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3379 - val_loss: 0.2999\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.3117 - val_loss: 0.3611\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.3580 - val_loss: 0.5178\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.4346 - val_loss: 0.5082\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.3623 - val_loss: 0.1126\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.2329 - val_loss: 0.2528\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3744 - val_loss: 0.5249\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.5433 - val_loss: 0.6451\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.6277 - val_loss: 0.7522\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.8968 - val_loss: 0.2544\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.5890 - val_loss: 0.2730\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.6141 - val_loss: 0.9529\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.6640 - val_loss: 0.9491\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.6651 - val_loss: 0.8196\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.8761 - val_loss: 0.2639\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.6035 - val_loss: 0.3083\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.4961 - val_loss: 0.5148\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.4623 - val_loss: 0.5040\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.3655 - val_loss: 0.2324\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3750 - val_loss: 0.2074\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2923 - val_loss: 0.2863\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.2852 - val_loss: 0.2466\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.2016 - val_loss: 0.1561\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1631 - val_loss: 0.1319\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.1556 - val_loss: 0.4228\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.3400 - val_loss: 0.5002\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.3793 - val_loss: 0.5898\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3967 - val_loss: 0.4958\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.3890 - val_loss: 0.8379\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.5850 - val_loss: 0.6622\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.5173 - val_loss: 0.6207\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.5008 - val_loss: 0.4221\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.3828 - val_loss: 0.4154\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.3165 - val_loss: 0.1862\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.1825 - val_loss: 0.1893\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.2020 - val_loss: 0.1031\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.1437 - val_loss: 0.1563\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1650 - val_loss: 0.1474\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.1420 - val_loss: 0.1304\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.1244 - val_loss: 0.0962\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.1072 - val_loss: 0.1303\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.1197 - val_loss: 0.1591\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.1795 - val_loss: 0.1839\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.1743 - val_loss: 0.2409\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.1778 - val_loss: 0.1093\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.1199 - val_loss: 0.1341\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.1679 - val_loss: 0.3945\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.3366 - val_loss: 0.2512\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.2194 - val_loss: 0.1148\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.1391 - val_loss: 0.0972\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.1296 - val_loss: 0.2249\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.1540 - val_loss: 0.1266\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.1055 - val_loss: 0.1227\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.1812 - val_loss: 0.3249\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2904 - val_loss: 0.3132\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.2141 - val_loss: 0.0973\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.1482 - val_loss: 0.1301\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best Model's Parameters\n",
      "{'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 2800, 'build_fn': <function build_model at 0x15075e280>}\n",
      "\n",
      " The mean absolute error (MAE)\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1301\n",
      "-0.13005216419696808\n",
      "\n",
      " The predicted values (Lattice constants)\n",
      "[[5.997917  8.33561   5.840996 ]\n",
      " [5.457684  7.5558767 5.2886724]\n",
      " [5.7209697 7.9216123 5.5437055]\n",
      " [5.6866207 7.8818374 5.5183363]\n",
      " [5.565908  7.6892633 5.376553 ]\n",
      " [5.5859737 7.7075396 5.3863544]\n",
      " [5.75904   7.982876  5.58891  ]\n",
      " [5.705564  7.9121237 5.540638 ]\n",
      " [5.628599  7.7748632 5.435798 ]\n",
      " [5.676988  7.8664613 5.507021 ]\n",
      " [5.6180205 7.758133  5.4235067]\n",
      " [5.534153  7.6389227 5.3395333]\n",
      " [5.6391354 7.7915306 5.448047 ]\n",
      " [5.757929  7.5449605 5.034639 ]\n",
      " [6.118395  8.499032  5.95371  ]\n",
      " [5.6391354 7.7915306 5.448047 ]\n",
      " [5.7209697 7.9216123 5.5437055]\n",
      " [6.397346  8.902639  6.2398148]\n",
      " [5.5323863 7.6651497 5.36652  ]\n",
      " [5.794753  8.030207  5.6212316]]\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "print (\"\\n History objects\")\n",
    "best_model_history = best_model.fit(X_train, y_train, epochs=n_epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=tolerance_value)])\n",
    "              \n",
    "plt.figure()\n",
    "plt.plot(best_model_history.history['loss'])\n",
    "plt.plot(best_model_history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.show()\n",
    "\n",
    "print (\"\\n Best Model's Parameters\")\n",
    "print (best_model.get_params())\n",
    "print (\"\\n The mean absolute error (MAE)\")\n",
    "print (best_model.score(X_test, y_test))\n",
    "print (\"\\n The predicted values (Lattice constants)\")\n",
    "print (best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-advice",
   "metadata": {
    "id": "PyWxHUQn92zG"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook too I deployed the ANN and found was successful in predicting the lattice constants with the mean absolute error around -0.05. The GridSearch did not perform well I would have expected it do. However, it along a few techniques like feature scaling and the commented out linear model is next task in my schedule. \n",
    "\n",
    "This notebook could not be amongst the best performer. However, especially after building an ANN I am motivated to work further on Machine Learning domain. I have all the incentives to move ahead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-vehicle",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Chonghe, Li, et al. \"Prediction of lattice constant in perovskites of GdFeO3 structure.\" Journal of Physics and Chemistry of Solids 64.11 (2003): 2147-2156.\n",
    "\n",
    "[2] Alade, Ibrahim Olanrewaju, Ismail Adewale Olumegbon, and Aliyu Bagudu. \"Lattice constant prediction of A2XY6 cubic crystals (A= K, Cs, Rb, TI; X= tetravalent cation; Y= F, Cl, Br, I) using computational intelligence approach.\" Journal of Applied Physics 127.1 (2020): 015303.\n",
    "\n",
    "[3] Jiang, L. Q., et al. \"Prediction of lattice constant in cubic perovskites.\" Journal of Physics and Chemistry of Solids 67.7 (2006): 1531-1536.\n",
    "\n",
    "[4] “Introduction to Machine Learning with Python, A Guide for Data Scientists\", by Andreas C. Müller and Sarah Guido\n",
    "\n",
    "[5] “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\", by Aurélien Géron\n",
    "\n",
    "[6] \"The Hundred-Page Machine Learning Book\", by Andriy Burkov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-queensland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ANN-ABO3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
