{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lesser-indicator",
   "metadata": {
    "id": "iraqi-thriller"
   },
   "source": [
    "## Introduction\n",
    "This notebook aims to replicate the studies that were successful in predicting the lattice constants of the Perovskite compound ($ABO_{3}$). The papers preoduced the results with the model's error around 2%. It's important to identify the features neccessary to perform predictions. The lattice constants of the $ABO_{3}$ -type may be correlated compounds as a general function of nine variables as below\n",
    "\n",
    "$Lattice constant = f(z_A + z_B + z_O + r_A + r_B + r_O + x_A + x_B + x_O)$\n",
    "\n",
    "where z, r and x denote the valence, the radius and the electronegativity of the ions (A, B, O), respectively. It is noted that the three variables associated with anion $O^{2-}$ , namely $z_{O}, r_{O},$ and $x_{O}$ can be ignored as they remain unchanged for all of the samples.\n",
    "\n",
    "The lattice constant can be reduced as a ﬁve-parameter function shown below.\n",
    "\n",
    "$Lattice constant = f(z_A + r_A + r_B + x_A + x_B)$\n",
    "\n",
    "There's a Goldschmidt’s tolerance factor (t), which can condense the atomic radii. It has been widely accepted as a criterion for the formation of the perovskite structure, up to now. I will use $\\textbf{four features}$ including the tolerance factor and others are denoted by, $r_{A}, r_{B}$ and $\\frac{r_{A}}{t}$. The list of feature does not include electronegativity and valence number. Basically, other sets of features that includes these two atomic configurations and they can be use to predict the lattice constant with great accuracy. However, I will only take the atomic radii in to my account. The tolerance factor (t) is expressed as below.\n",
    "\n",
    "$t = \\frac{r_A + r_O}{\\sqrt{2}(r_B + r_O)}$\n",
    "\n",
    "## Procedure\n",
    "First a simple regression model will be built which imitates the ANN (Built from TensorFlow APIs). Then the ANN model will be built to perform a GridSearch. Finally the predictions will be performed on the model with fine tuned hyperparameters. The GridSearch in the sklearn package provides the best model which can be used to fit and predict. \n",
    "\n",
    "## Feature engineering\n",
    "In the following section, the \".csv\" file is loaded into the program. The data is inspected for assurity. I observe the '.csv' provides the atomic radii for two atoms A and B in the crystal ($r_{A}$ and $r_{b}$). So the tolerance factor and $\\frac{r_{A}}{t}$ needs to be calculated. I have inserted them in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vocational-guard",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "actual-cemetery",
    "outputId": "6dbe1700-fcb6-4d60-b8f7-e8a6b217be5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five smaples from the DataFrame\n",
      "\n",
      "            ra    rb    xa    xb  za       a       b       c\n",
      "compound                                                    \n",
      "BaCeO3    1.35  0.87  0.89  1.12   2  6.2350  8.7810  6.2120\n",
      "BaPrO3    1.35  0.85  0.89  1.13   2  6.2140  8.7220  6.1810\n",
      "BaPuO3    1.35  0.86  0.89  1.30   2  6.1930  8.7440  6.2190\n",
      "CaCrO3    1.00  0.55  1.00  1.66   2  5.3160  7.4860  5.2870\n",
      "CaGeO3    1.00  0.53  1.00  2.01   2  5.2688  7.4452  5.2607\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "lat_df = pd.read_csv('lat_const.csv', header=0, index_col = 0, usecols= ['compound','ra','rb','xa','xb','za','a','b','c'])\n",
    "print (\"The first five smaples from the DataFrame\\n\")\n",
    "print (lat_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "external-profession",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "editorial-provider",
    "outputId": "759b4126-9bf4-4762-c1a1-dc53a770cdd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five rows of imputed DataFrame\n",
      "\n",
      "            ra    rb    xa    xb  za       a       b       c  tolerance  \\\n",
      "compound                                                                  \n",
      "BaCeO3    1.35  0.87  0.89  1.12   2  6.2350  8.7810  6.2120   0.856627   \n",
      "BaPrO3    1.35  0.85  0.89  1.13   2  6.2140  8.7220  6.1810   0.864242   \n",
      "BaPuO3    1.35  0.86  0.89  1.30   2  6.1930  8.7440  6.2190   0.860418   \n",
      "CaCrO3    1.00  0.55  1.00  1.66   2  5.3160  7.4860  5.2870   0.870285   \n",
      "CaGeO3    1.00  0.53  1.00  2.01   2  5.2688  7.4452  5.2607   0.879304   \n",
      "\n",
      "               rat  \n",
      "compound            \n",
      "BaCeO3    1.575948  \n",
      "BaPrO3    1.562063  \n",
      "BaPuO3    1.569006  \n",
      "CaCrO3    1.149049  \n",
      "CaGeO3    1.137263  \n",
      "\n",
      "The first five observations of the features\n",
      "[[1.35       0.87       0.85662717 1.5759481 ]\n",
      " [1.35       0.85       0.8642416  1.5620632 ]\n",
      " [1.35       0.86       0.86041754 1.5690056 ]\n",
      " [1.         0.55       0.8702853  1.1490486 ]\n",
      " [1.         0.53       0.87930375 1.1372634 ]]\n",
      "\n",
      "The first five observations of the targets\n",
      "[[6.235  8.781  6.212 ]\n",
      " [6.214  8.722  6.181 ]\n",
      " [6.193  8.744  6.219 ]\n",
      " [5.316  7.486  5.287 ]\n",
      " [5.2688 7.4452 5.2607]]\n"
     ]
    }
   ],
   "source": [
    "lat_df = lat_df.assign(tolerance = lambda x:((x['ra'] + 1.4)/(np.sqrt(2)*(x['rb'] + 1.4))))\n",
    "lat_df = lat_df.assign(rat = lambda x:(x['ra']/x['tolerance']))\n",
    "\n",
    "print (\"The first five rows of imputed DataFrame\\n\")\n",
    "print (lat_df.head())\n",
    "\n",
    "features = np.array(lat_df[['ra', 'rb', 'tolerance', 'rat']], np.float32)\n",
    "targets = np.array(lat_df[['a', 'b', 'c']], np.float32)\n",
    "\n",
    "print (\"\\nThe first five observations of the features\")\n",
    "print (features[:5,:])\n",
    "print (\"\\nThe first five observations of the targets\")\n",
    "print (targets[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-anthony",
   "metadata": {},
   "source": [
    "## Linear Regression model\n",
    "Here I have built a simple LR model. The weight and the bias is defined as tensor. The two functions \"linear_regression\" and \"loss_function\" are coded without any major assistance. The goal will be to optimize the loss function. Once optimized the bias and the weights can be extracted to predict the lattice constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "italic-quantity",
   "metadata": {
    "id": "interracial-southwest"
   },
   "outputs": [],
   "source": [
    "# # W = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "# # b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "\n",
    "# W = tf.Variable(np.array([[0.1, 0.1, 0.1]] *4), name = \"W\", shape=tf.TensorShape([4,3]))\n",
    "# b = tf.Variable(np.array([[0.1, 0.1, 0.1]] *100), name = \"b\", shape=tf.TensorShape([100,3]))\n",
    "\n",
    "# def linear_regression(intercept, slope):\n",
    "#     return tf.add(tf.multiply(features,slope), intercept)\n",
    "\n",
    "# def loss_function(intercept, slope):\n",
    "#     predictions = linear_regression(intercept, slope)\n",
    "#     return tf.keras.losses.mse(targets, predictions)\n",
    "\n",
    "# opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# for j in range(500):\n",
    "#     opt.minimize(lambda: loss_function(b, W), var_list= [b, W])\n",
    "#     print (loss_function(intercept, slope))\n",
    "    \n",
    "# print (\"Intercept and Slope from the optimization\")\n",
    "# print (intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-russia",
   "metadata": {},
   "source": [
    "## Machine Learning Model\n",
    "I will build an artificial neural network (Hereafter called as ANN). The loss function will be mean square error. \"MSE\" greatly penalised the outliers in predictions compared to mean absolute error and huber loss function. I will choose Adaptive optimizer over Stochastic gradient descent (SGD). The ANN has lots of hyperparameters to be tuned before I set out to predict the target variables. Besides Adam optimizer the other hyperparameters are number of neural nodes, number of hidden layers, learning rate (Adam optimizer), activation functions and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interior-brighton",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "weird-offense",
    "outputId": "21326357-b249-4129-816b-cfd4692d28d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 1s 65ms/step - loss: 6.0795 - val_loss: 5.7420\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 5.6131 - val_loss: 5.2612\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 5.1105 - val_loss: 4.6708\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 4.4764 - val_loss: 3.9003\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 3.6510 - val_loss: 2.8975\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.5729 - val_loss: 1.6301\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.4782 - val_loss: 1.4552\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.4299 - val_loss: 1.2948\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.1649 - val_loss: 0.9894\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8913 - val_loss: 0.5414\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4017 - val_loss: 0.4781\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5360 - val_loss: 0.6645\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6110 - val_loss: 0.4664\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3855 - val_loss: 0.3408\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3343 - val_loss: 0.3656\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2990 - val_loss: 0.3287\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3174 - val_loss: 0.3679\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3060 - val_loss: 0.2634\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2591 - val_loss: 0.3168\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2599 - val_loss: 0.2838\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2431 - val_loss: 0.2670\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2235 - val_loss: 0.2723\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2307 - val_loss: 0.2544\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2150 - val_loss: 0.2663\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2199 - val_loss: 0.2540\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2126 - val_loss: 0.2438\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2068 - val_loss: 0.2428\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2016 - val_loss: 0.2498\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2062 - val_loss: 0.2473\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1992 - val_loss: 0.2386\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1960 - val_loss: 0.2415\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1992 - val_loss: 0.2360\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1909 - val_loss: 0.2349\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1909 - val_loss: 0.2318\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1900 - val_loss: 0.2274\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1861 - val_loss: 0.2272\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1899 - val_loss: 0.2224\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1806 - val_loss: 0.2305\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1871 - val_loss: 0.2233\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1892 - val_loss: 0.2150\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1830 - val_loss: 0.2149\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1763 - val_loss: 0.2129\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1780 - val_loss: 0.2076\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1733 - val_loss: 0.2021\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1771 - val_loss: 0.2083\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1746 - val_loss: 0.2040\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1682 - val_loss: 0.2005\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1700 - val_loss: 0.1955\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1633 - val_loss: 0.1941\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1710 - val_loss: 0.1904\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1622 - val_loss: 0.1922\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1623 - val_loss: 0.1865\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1557 - val_loss: 0.1835\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1554 - val_loss: 0.1834\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1585 - val_loss: 0.1840\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1528 - val_loss: 0.1809\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1519 - val_loss: 0.1726\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1504 - val_loss: 0.1682\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1452 - val_loss: 0.1661\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1437 - val_loss: 0.1697\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1467 - val_loss: 0.1666\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1438 - val_loss: 0.1623\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1420 - val_loss: 0.1622\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1408 - val_loss: 0.1562\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1409 - val_loss: 0.1551\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1416 - val_loss: 0.1493\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1339 - val_loss: 0.1503\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1357 - val_loss: 0.1449\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1326 - val_loss: 0.1447\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1348 - val_loss: 0.1459\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1284 - val_loss: 0.1374\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1238 - val_loss: 0.1324\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1249 - val_loss: 0.1370\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1264 - val_loss: 0.1279\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1155 - val_loss: 0.1254\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1113 - val_loss: 0.1418\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1244 - val_loss: 0.1345\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1258 - val_loss: 0.1301\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1132 - val_loss: 0.1242\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1154 - val_loss: 0.1132\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1045 - val_loss: 0.1124\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1033 - val_loss: 0.1082\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1048 - val_loss: 0.1064\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0984 - val_loss: 0.1024\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0976 - val_loss: 0.1019\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0993 - val_loss: 0.0975\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0946 - val_loss: 0.0957\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0912 - val_loss: 0.0966\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0930 - val_loss: 0.0986\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0953 - val_loss: 0.0914\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0889 - val_loss: 0.0855\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0923 - val_loss: 0.0841\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0861 - val_loss: 0.0893\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0870 - val_loss: 0.0887\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0959 - val_loss: 0.0825\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0890 - val_loss: 0.0948\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0945 - val_loss: 0.0856\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0880 - val_loss: 0.0768\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0861 - val_loss: 0.0765\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0825 - val_loss: 0.0742\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0857 - val_loss: 0.0769\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0804 - val_loss: 0.0689\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0718 - val_loss: 0.0710\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0746 - val_loss: 0.0654\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0726 - val_loss: 0.0804\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0803 - val_loss: 0.0753\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0784 - val_loss: 0.0675\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0711 - val_loss: 0.0708\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0748 - val_loss: 0.0681\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0793 - val_loss: 0.0777\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0804 - val_loss: 0.0700\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0681 - val_loss: 0.0644\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0673 - val_loss: 0.0695\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0693 - val_loss: 0.0657\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0719 - val_loss: 0.0683\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0720 - val_loss: 0.0797\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0769 - val_loss: 0.0627\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0673 - val_loss: 0.0653\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0700 - val_loss: 0.0632\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0766 - val_loss: 0.0693\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0752 - val_loss: 0.0860\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0858 - val_loss: 0.0718\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0739 - val_loss: 0.0630\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0694 - val_loss: 0.0624\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0683 - val_loss: 0.0704\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0708 - val_loss: 0.0662\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0693 - val_loss: 0.0629\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0663 - val_loss: 0.0635\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0672 - val_loss: 0.0590\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0645 - val_loss: 0.0618\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0654 - val_loss: 0.0722\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0788 - val_loss: 0.0840\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0875 - val_loss: 0.0834\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0811 - val_loss: 0.0842\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0914 - val_loss: 0.1172\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1014 - val_loss: 0.1076\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1025 - val_loss: 0.1160\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0981 - val_loss: 0.1287\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1095 - val_loss: 0.0856\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0931 - val_loss: 0.0801\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0825 - val_loss: 0.0837\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0923 - val_loss: 0.0952\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0905 - val_loss: 0.0942\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0878 - val_loss: 0.0820\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0789 - val_loss: 0.0561\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0696 - val_loss: 0.0730\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0749 - val_loss: 0.0699\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0674 - val_loss: 0.0556\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0650 - val_loss: 0.0650\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0716 - val_loss: 0.0616\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0660 - val_loss: 0.0578\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0657 - val_loss: 0.0614\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0630 - val_loss: 0.0549\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0638 - val_loss: 0.0574\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0612 - val_loss: 0.0576\n",
      "Epoch 156/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0636 - val_loss: 0.0661\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0722 - val_loss: 0.0692\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0705 - val_loss: 0.0552\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0628 - val_loss: 0.0587\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0641 - val_loss: 0.0559\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0641 - val_loss: 0.0622\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0647 - val_loss: 0.0645\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0711 - val_loss: 0.0625\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0704 - val_loss: 0.0631\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0653 - val_loss: 0.0625\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0656 - val_loss: 0.0617\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0657 - val_loss: 0.0577\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0701 - val_loss: 0.0593\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0646 - val_loss: 0.0602\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0669 - val_loss: 0.0645\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0674 - val_loss: 0.0579\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0618 - val_loss: 0.0568\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0595 - val_loss: 0.0712\n",
      "Epoch 174/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0655 - val_loss: 0.0536\n",
      "Epoch 175/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0638 - val_loss: 0.0733\n",
      "Epoch 176/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0767 - val_loss: 0.0860\n",
      "Epoch 177/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0784 - val_loss: 0.0556\n",
      "Epoch 178/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0683 - val_loss: 0.0723\n",
      "Epoch 179/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0825 - val_loss: 0.0820\n",
      "Epoch 180/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0861 - val_loss: 0.0649\n",
      "Epoch 181/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0754 - val_loss: 0.0844\n",
      "Epoch 182/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0899 - val_loss: 0.0739\n",
      "Epoch 183/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0747 - val_loss: 0.0605\n",
      "Epoch 184/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0654 - val_loss: 0.0623\n",
      "Epoch 185/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0676 - val_loss: 0.0558\n",
      "Epoch 186/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0632 - val_loss: 0.0582\n",
      "Epoch 187/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0624 - val_loss: 0.0547\n",
      "Epoch 188/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0691 - val_loss: 0.0624\n",
      "Epoch 189/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0715 - val_loss: 0.0564\n",
      "Epoch 190/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0610 - val_loss: 0.0591\n",
      "Epoch 191/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0614 - val_loss: 0.0542\n",
      "Epoch 192/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0616 - val_loss: 0.0604\n",
      "Epoch 193/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0720 - val_loss: 0.0602\n",
      "Epoch 194/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0644 - val_loss: 0.0579\n",
      "Epoch 195/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0670 - val_loss: 0.0610\n",
      "Epoch 196/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0684 - val_loss: 0.0632\n",
      "Epoch 197/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0764 - val_loss: 0.0677\n",
      "Epoch 198/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0711 - val_loss: 0.0647\n",
      "Epoch 199/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0711 - val_loss: 0.0638\n",
      "Epoch 200/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0715 - val_loss: 0.0707\n",
      "Epoch 201/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0696 - val_loss: 0.0765\n",
      "Epoch 202/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0764 - val_loss: 0.0776\n",
      "Epoch 203/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0743 - val_loss: 0.0630\n",
      "Epoch 204/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0780 - val_loss: 0.0546\n",
      "Epoch 205/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0676 - val_loss: 0.0579\n",
      "Epoch 206/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0616 - val_loss: 0.0649\n",
      "Epoch 207/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0636 - val_loss: 0.0592\n",
      "Epoch 208/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0648 - val_loss: 0.0562\n",
      "Epoch 209/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0617 - val_loss: 0.0638\n",
      "Epoch 210/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0686 - val_loss: 0.0535\n",
      "Epoch 211/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0625 - val_loss: 0.0990\n",
      "Epoch 212/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0896 - val_loss: 0.1009\n",
      "Epoch 213/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0949 - val_loss: 0.0874\n",
      "Epoch 214/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0847 - val_loss: 0.0549\n",
      "Epoch 215/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0742 - val_loss: 0.0760\n",
      "Epoch 216/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0850 - val_loss: 0.1138\n",
      "Epoch 217/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1023 - val_loss: 0.0785\n",
      "Epoch 218/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0758 - val_loss: 0.0554\n",
      "Epoch 219/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0796 - val_loss: 0.0746\n",
      "Epoch 220/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0799 - val_loss: 0.0683\n",
      "Epoch 221/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0700 - val_loss: 0.0617\n",
      "Epoch 222/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0724 - val_loss: 0.0581\n",
      "Epoch 223/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0643 - val_loss: 0.0627\n",
      "Epoch 224/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0680 - val_loss: 0.0529\n",
      "Epoch 225/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0656 - val_loss: 0.0624\n",
      "Epoch 226/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0644 - val_loss: 0.0527\n",
      "Epoch 227/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0611 - val_loss: 0.0708\n",
      "Epoch 228/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0756 - val_loss: 0.0731\n",
      "Epoch 229/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0690 - val_loss: 0.0626\n",
      "Epoch 230/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0706 - val_loss: 0.0579\n",
      "Epoch 231/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0647 - val_loss: 0.0718\n",
      "Epoch 232/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0714 - val_loss: 0.0532\n",
      "Epoch 233/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0625 - val_loss: 0.0560\n",
      "Epoch 234/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0645 - val_loss: 0.0542\n",
      "Epoch 235/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0583 - val_loss: 0.0554\n",
      "Epoch 236/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0601 - val_loss: 0.0551\n",
      "Epoch 237/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0590 - val_loss: 0.0571\n",
      "Epoch 238/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0660 - val_loss: 0.0580\n",
      "Epoch 239/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0692 - val_loss: 0.0517\n",
      "Epoch 240/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0655 - val_loss: 0.0681\n",
      "Epoch 241/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0787 - val_loss: 0.0664\n",
      "Epoch 242/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0688 - val_loss: 0.0678\n",
      "Epoch 243/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0687 - val_loss: 0.0774\n",
      "Epoch 244/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0775 - val_loss: 0.0839\n",
      "Epoch 245/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0753 - val_loss: 0.0513\n",
      "Epoch 246/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0665 - val_loss: 0.0680\n",
      "Epoch 247/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0745 - val_loss: 0.0820\n",
      "Epoch 248/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0760 - val_loss: 0.0659\n",
      "Epoch 249/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0695 - val_loss: 0.0570\n",
      "Epoch 250/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0660 - val_loss: 0.0575\n",
      "Epoch 251/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0639 - val_loss: 0.0601\n",
      "Epoch 252/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0635 - val_loss: 0.0528\n",
      "Epoch 253/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0617 - val_loss: 0.0515\n",
      "Epoch 254/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0578 - val_loss: 0.0601\n",
      "Epoch 255/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0615 - val_loss: 0.0630\n",
      "Epoch 256/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0712 - val_loss: 0.0531\n",
      "Epoch 257/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0651 - val_loss: 0.0705\n",
      "Epoch 258/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0715 - val_loss: 0.0535\n",
      "Epoch 259/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0655 - val_loss: 0.0539\n",
      "Epoch 260/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0620 - val_loss: 0.0506\n",
      "Epoch 261/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0575 - val_loss: 0.0737\n",
      "Epoch 262/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0702 - val_loss: 0.0547\n",
      "Epoch 263/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0658 - val_loss: 0.0557\n",
      "Epoch 264/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0611 - val_loss: 0.0590\n",
      "Epoch 265/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0607 - val_loss: 0.0529\n",
      "Epoch 266/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0602 - val_loss: 0.0529\n",
      "Epoch 267/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0602 - val_loss: 0.0505\n",
      "Epoch 268/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0583 - val_loss: 0.0609\n",
      "Epoch 269/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0621 - val_loss: 0.0595\n",
      "Epoch 270/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0643 - val_loss: 0.0522\n",
      "Epoch 271/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0587 - val_loss: 0.0717\n",
      "Epoch 272/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0677 - val_loss: 0.0531\n",
      "Epoch 273/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0601 - val_loss: 0.0542\n",
      "Epoch 274/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0625 - val_loss: 0.0490\n",
      "Epoch 275/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0570 - val_loss: 0.0503\n",
      "Epoch 276/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0569 - val_loss: 0.0491\n",
      "Epoch 277/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0559 - val_loss: 0.0571\n",
      "Epoch 278/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0618 - val_loss: 0.0564\n",
      "Epoch 279/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0704 - val_loss: 0.0748\n",
      "Epoch 280/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0771 - val_loss: 0.0690\n",
      "Epoch 281/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0748 - val_loss: 0.0710\n",
      "Epoch 282/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0736 - val_loss: 0.0534\n",
      "Epoch 283/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0607 - val_loss: 0.0508\n",
      "Epoch 284/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0587 - val_loss: 0.0486\n",
      "Epoch 285/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0632 - val_loss: 0.0517\n",
      "Epoch 286/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0569 - val_loss: 0.0498\n",
      "Epoch 287/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0583 - val_loss: 0.0486\n",
      "Epoch 288/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0562 - val_loss: 0.0519\n",
      "Epoch 289/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0581 - val_loss: 0.0615\n",
      "Epoch 290/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0638 - val_loss: 0.0543\n",
      "Epoch 291/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0606 - val_loss: 0.0550\n",
      "Epoch 292/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0616 - val_loss: 0.0941\n",
      "Epoch 293/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0815 - val_loss: 0.0728\n",
      "Epoch 294/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0777 - val_loss: 0.0517\n",
      "Epoch 295/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0575 - val_loss: 0.0494\n",
      "Epoch 296/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0562 - val_loss: 0.0506\n",
      "Epoch 297/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0571 - val_loss: 0.0508\n",
      "Epoch 298/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0595 - val_loss: 0.0508\n",
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0527\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0588 - val_loss: 0.0494\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0565 - val_loss: 0.0524\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0597 - val_loss: 0.0699\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0665 - val_loss: 0.0489\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0592 - val_loss: 0.0556\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0635 - val_loss: 0.0584\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0649 - val_loss: 0.0500\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0605 - val_loss: 0.0588\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0621 - val_loss: 0.0494\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0576 - val_loss: 0.0549\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0594 - val_loss: 0.0537\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0597 - val_loss: 0.0485\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0559 - val_loss: 0.0482\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0583 - val_loss: 0.0538\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0604 - val_loss: 0.0502\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0621 - val_loss: 0.0836\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0731 - val_loss: 0.0567\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0652 - val_loss: 0.0516\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0642 - val_loss: 0.0482\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0561 - val_loss: 0.0490\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0621 - val_loss: 0.0785\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0800 - val_loss: 0.0541\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0634 - val_loss: 0.0496\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0635 - val_loss: 0.0736\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0755 - val_loss: 0.0557\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0639 - val_loss: 0.0494\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0557 - val_loss: 0.0497\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0558 - val_loss: 0.0512\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0588 - val_loss: 0.0484\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0577 - val_loss: 0.0541\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0627 - val_loss: 0.0494\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0591 - val_loss: 0.0637\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0656 - val_loss: 0.0550\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0595 - val_loss: 0.0567\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0601 - val_loss: 0.0483\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0600 - val_loss: 0.0609\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0628 - val_loss: 0.0528\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0575 - val_loss: 0.0515\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0636 - val_loss: 0.0502\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0603 - val_loss: 0.0480\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0562 - val_loss: 0.0477\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0547 - val_loss: 0.0620\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0637 - val_loss: 0.0479\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0557 - val_loss: 0.0532\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0595 - val_loss: 0.0519\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0649 - val_loss: 0.0504\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0649 - val_loss: 0.0471\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0595 - val_loss: 0.0597\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0651 - val_loss: 0.0590\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0625 - val_loss: 0.0589\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0635 - val_loss: 0.0514\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0633 - val_loss: 0.0501\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0568 - val_loss: 0.0542\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0637 - val_loss: 0.0540\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0590 - val_loss: 0.0553\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0620 - val_loss: 0.0479\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0575 - val_loss: 0.0466\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0553 - val_loss: 0.0578\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0618 - val_loss: 0.0589\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0639 - val_loss: 0.0499\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0596 - val_loss: 0.0496\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.0603 - val_loss: 0.0549\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0634 - val_loss: 0.0541\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 50ms/step - loss: 0.0611 - val_loss: 0.0543\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0622 - val_loss: 0.0524\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0598 - val_loss: 0.0498\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0580 - val_loss: 0.0513\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 44ms/step - loss: 0.0582 - val_loss: 0.0495\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 0.0599 - val_loss: 0.0482\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.0589 - val_loss: 0.0483\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0577 - val_loss: 0.0521\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0598 - val_loss: 0.0458\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0602 - val_loss: 0.0653\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0671 - val_loss: 0.0588\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0681 - val_loss: 0.0480\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0619 - val_loss: 0.0780\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.0789 - val_loss: 0.0805\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0786 - val_loss: 0.0501\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0609 - val_loss: 0.0583\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0651 - val_loss: 0.0758\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0731 - val_loss: 0.0764\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0746 - val_loss: 0.0592\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0719 - val_loss: 0.0745\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0815 - val_loss: 0.1032\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0862 - val_loss: 0.0977\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0911 - val_loss: 0.1058\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0870 - val_loss: 0.0745\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0816 - val_loss: 0.0474\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0702 - val_loss: 0.0709\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0731 - val_loss: 0.0527\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.0637 - val_loss: 0.0524\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0598 - val_loss: 0.0589\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0630 - val_loss: 0.0456\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0560 - val_loss: 0.0511\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0621 - val_loss: 0.0521\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0611 - val_loss: 0.0493\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0609 - val_loss: 0.0557\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0626 - val_loss: 0.0679\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0676 - val_loss: 0.0530\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0667 - val_loss: 0.0459\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0626 - val_loss: 0.0506\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0618 - val_loss: 0.0585\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0611 - val_loss: 0.0508\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0602 - val_loss: 0.0646\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0714 - val_loss: 0.0635\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0678 - val_loss: 0.0482\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0568 - val_loss: 0.0527\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0613 - val_loss: 0.0506\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0629 - val_loss: 0.0681\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0696 - val_loss: 0.0593\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0647 - val_loss: 0.0492\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0648 - val_loss: 0.0540\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0597 - val_loss: 0.0503\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0609 - val_loss: 0.0511\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0563 - val_loss: 0.0544\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0661 - val_loss: 0.0471\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0555\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0599 - val_loss: 0.0480\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0562 - val_loss: 0.0677\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0612 - val_loss: 0.0626\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0669 - val_loss: 0.0447\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0588 - val_loss: 0.0561\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0599 - val_loss: 0.0454\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0612 - val_loss: 0.0524\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0587 - val_loss: 0.0452\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0624 - val_loss: 0.0760\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0757 - val_loss: 0.0608\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0700 - val_loss: 0.0476\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0621\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0626 - val_loss: 0.0507\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0633 - val_loss: 0.0549\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0664 - val_loss: 0.0443\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0589 - val_loss: 0.0609\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0624 - val_loss: 0.0538\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0667 - val_loss: 0.0629\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0700 - val_loss: 0.0808\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0723 - val_loss: 0.0794\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0752 - val_loss: 0.0471\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0608 - val_loss: 0.0576\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0625 - val_loss: 0.0466\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0626 - val_loss: 0.0473\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0555 - val_loss: 0.0557\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0631 - val_loss: 0.0458\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0561 - val_loss: 0.0456\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0555 - val_loss: 0.0434\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0538 - val_loss: 0.0437\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0552 - val_loss: 0.0460\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0583 - val_loss: 0.0449\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0561 - val_loss: 0.0464\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0562 - val_loss: 0.0444\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0546 - val_loss: 0.0470\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0537 - val_loss: 0.0565\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0647 - val_loss: 0.0467\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0644 - val_loss: 0.0644\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0711 - val_loss: 0.0645\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0674 - val_loss: 0.0438\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0581 - val_loss: 0.0528\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0605 - val_loss: 0.0430\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0609\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0643 - val_loss: 0.0500\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0596 - val_loss: 0.0504\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0579 - val_loss: 0.0569\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0645 - val_loss: 0.0468\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0745 - val_loss: 0.0564\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0667 - val_loss: 0.0467\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0452\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0573 - val_loss: 0.0516\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0630 - val_loss: 0.0515\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0600 - val_loss: 0.0671\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0688 - val_loss: 0.0575\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0674 - val_loss: 0.0469\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0626 - val_loss: 0.0663\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0683 - val_loss: 0.0575\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0664 - val_loss: 0.0454\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0590 - val_loss: 0.0719\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0718 - val_loss: 0.0661\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0689 - val_loss: 0.0683\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0707 - val_loss: 0.0450\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0591 - val_loss: 0.0649\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0701 - val_loss: 0.0427\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0580 - val_loss: 0.0494\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0588 - val_loss: 0.0522\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0617 - val_loss: 0.0470\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0598 - val_loss: 0.0670\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0610 - val_loss: 0.0784\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0727 - val_loss: 0.0530\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0664 - val_loss: 0.0508\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0666 - val_loss: 0.0661\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0730 - val_loss: 0.0654\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0677 - val_loss: 0.0633\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0628 - val_loss: 0.0466\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0630 - val_loss: 0.0854\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0835 - val_loss: 0.0946\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0846 - val_loss: 0.0801\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0766 - val_loss: 0.0577\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0806 - val_loss: 0.0466\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0661 - val_loss: 0.0715\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0746 - val_loss: 0.0614\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0635 - val_loss: 0.0504\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0551 - val_loss: 0.0542\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0598 - val_loss: 0.0492\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.0492\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The predicted values (Lattice constants)\n",
      "[[5.743847  8.092787  5.748471 ]\n",
      " [5.3093634 7.358053  5.1802735]\n",
      " [5.583061  7.7088337 5.41275  ]\n",
      " [5.5117407 7.6634083 5.4036994]\n",
      " [5.4994097 7.5055833 5.233827 ]\n",
      " [5.5708876 7.538664  5.2287507]\n",
      " [5.5865593 7.7604165 5.468516 ]\n",
      " [5.5136986 7.689405  5.4316745]\n",
      " [5.574771  7.5914583 5.2858405]\n",
      " [5.5107527 7.6503797 5.3896775]\n",
      " [5.5738125 7.5782976 5.27161  ]\n",
      " [5.496156  7.4655933 5.190776 ]\n",
      " [5.575722  7.6045947 5.300045 ]\n",
      " [5.836048  7.4996676 4.9475436]\n",
      " [5.9173174 8.263741  5.838536 ]\n",
      " [5.575722  7.6045947 5.300045 ]\n",
      " [5.583061  7.7088337 5.41275  ]\n",
      " [6.1278343 8.666522  6.1717196]\n",
      " [5.3621197 7.4577937 5.2611027]\n",
      " [5.6356134 7.8083878 5.4935083]]\n",
      "\n",
      " The mean absolute error (MAE)\n",
      "-0.04916026443243027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The seed is required for replication of results. Parameters defined.\n",
    "seed = 1111\n",
    "tolerance_value = 45\n",
    "n_epochs = 500\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets,\n",
    "                                                    random_state=seed,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "def build_model(n_hidden=2, n_neurons=400, learning_rate=1e-3, input_shape=[4,]):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(3))\n",
    "\n",
    "    model.compile(loss=\"mae\", optimizer=keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    return model\n",
    "\n",
    "# The wrapper is neccessary for GridSearch later in this notebook\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "\n",
    "# The history dictionary is useful for 'loss' plot of the model. In this case, the 'loss' is mean absolute error \n",
    "history = keras_reg.fit(X_train, y_train, epochs=n_epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=tolerance_value)])\n",
    "\n",
    "# Test on the holdout data\n",
    "mae_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print (\"\\n The predicted values (Lattice constants)\")\n",
    "print (y_pred)\n",
    "print (\"\\n The mean absolute error (MAE)\")\n",
    "print (mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-cooperative",
   "metadata": {
    "id": "atlantic-arrow"
   },
   "source": [
    "## GridSearch\n",
    "Grid search (GS) as the name suggests requires a grid made up by the hyperparamters. The alogorithm then runs the model over the grid and finds the best set of parameters for which the model perform best, i.e. low error (MSE, MAE, huber loss, etc in case of refression). The algorithm is heavy because it is performing a brute force approach. There are some techniques like Bayesian inference methods which are very quick in findig the hyperparameters. \n",
    "\n",
    "In the GS, I will set cross-validation option which results in accurate loss metrics. Cross-validation is a statistical method of evaluating generalization performance that is more stable and thorough than using a split into a training and a test set.\n",
    "\n",
    "The parameter grid for my models are bulleted below.\n",
    "\n",
    "1.   number of hidden layers (n_hidden)\n",
    "2.   number of neural nodes in each hidden layers (n_neurons)\n",
    "3.   learning rate for Adam optimizer\n",
    "\n",
    "Since GS is a type of brute force approach I have employed a callback fucntion which will terminate the GS once the patience level is crossed. This way I do not have to lose the computational power once the model loss starts widening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "threatened-bridal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y2FNniocgLzQ",
    "outputId": "46dbc949-688d-4577-c759-1fedbe6a267b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 59ms/step - loss: 6.0261 - val_loss: 5.2820\n",
      "Best: -5.658186 using {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 600}\n",
      "mean: -6.126259, std: (0.014495) parameters: {'learning_rate': 0.001, 'n_hidden': 1, 'n_neurons': 300}\n",
      "mean: -6.189782, std: (0.070117) parameters: {'learning_rate': 0.001, 'n_hidden': 1, 'n_neurons': 400}\n",
      "mean: -6.157192, std: (0.022201) parameters: {'learning_rate': 0.001, 'n_hidden': 1, 'n_neurons': 500}\n",
      "mean: -6.159222, std: (0.059019) parameters: {'learning_rate': 0.001, 'n_hidden': 1, 'n_neurons': 600}\n",
      "mean: -5.955916, std: (0.079322) parameters: {'learning_rate': 0.001, 'n_hidden': 2, 'n_neurons': 300}\n",
      "mean: -5.916473, std: (0.046287) parameters: {'learning_rate': 0.001, 'n_hidden': 2, 'n_neurons': 400}\n",
      "mean: -5.807509, std: (0.049166) parameters: {'learning_rate': 0.001, 'n_hidden': 2, 'n_neurons': 500}\n",
      "mean: -5.790674, std: (0.068013) parameters: {'learning_rate': 0.001, 'n_hidden': 2, 'n_neurons': 600}\n",
      "mean: -5.957816, std: (0.030666) parameters: {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 300}\n",
      "mean: -5.853539, std: (0.038135) parameters: {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 400}\n",
      "mean: -5.813362, std: (0.030396) parameters: {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 500}\n",
      "mean: -5.658186, std: (0.037364) parameters: {'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 600}\n",
      "mean: -5.898802, std: (0.023821) parameters: {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 300}\n",
      "mean: -5.931351, std: (0.044718) parameters: {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 400}\n",
      "mean: -5.749099, std: (0.088434) parameters: {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 500}\n",
      "mean: -5.739570, std: (0.057260) parameters: {'learning_rate': 0.001, 'n_hidden': 4, 'n_neurons': 600}\n",
      "mean: -6.232770, std: (0.036830) parameters: {'learning_rate': 0.0001, 'n_hidden': 1, 'n_neurons': 300}\n",
      "mean: -6.245386, std: (0.105129) parameters: {'learning_rate': 0.0001, 'n_hidden': 1, 'n_neurons': 400}\n",
      "mean: -6.258605, std: (0.088094) parameters: {'learning_rate': 0.0001, 'n_hidden': 1, 'n_neurons': 500}\n",
      "mean: -6.273526, std: (0.031711) parameters: {'learning_rate': 0.0001, 'n_hidden': 1, 'n_neurons': 600}\n",
      "mean: -6.295913, std: (0.044897) parameters: {'learning_rate': 0.0001, 'n_hidden': 2, 'n_neurons': 300}\n",
      "mean: -6.205027, std: (0.042543) parameters: {'learning_rate': 0.0001, 'n_hidden': 2, 'n_neurons': 400}\n",
      "mean: -6.214991, std: (0.028822) parameters: {'learning_rate': 0.0001, 'n_hidden': 2, 'n_neurons': 500}\n",
      "mean: -6.235237, std: (0.036625) parameters: {'learning_rate': 0.0001, 'n_hidden': 2, 'n_neurons': 600}\n",
      "mean: -6.212933, std: (0.026144) parameters: {'learning_rate': 0.0001, 'n_hidden': 3, 'n_neurons': 300}\n",
      "mean: -6.237224, std: (0.035153) parameters: {'learning_rate': 0.0001, 'n_hidden': 3, 'n_neurons': 400}\n",
      "mean: -6.189657, std: (0.011719) parameters: {'learning_rate': 0.0001, 'n_hidden': 3, 'n_neurons': 500}\n",
      "mean: -6.227330, std: (0.026716) parameters: {'learning_rate': 0.0001, 'n_hidden': 3, 'n_neurons': 600}\n",
      "mean: -6.215673, std: (0.019778) parameters: {'learning_rate': 0.0001, 'n_hidden': 4, 'n_neurons': 300}\n",
      "mean: -6.228803, std: (0.041141) parameters: {'learning_rate': 0.0001, 'n_hidden': 4, 'n_neurons': 400}\n",
      "mean: -6.202054, std: (0.014320) parameters: {'learning_rate': 0.0001, 'n_hidden': 4, 'n_neurons': 500}\n",
      "mean: -6.199507, std: (0.013873) parameters: {'learning_rate': 0.0001, 'n_hidden': 4, 'n_neurons': 600}\n",
      "\n",
      " History objects\n",
      "Epoch 1/500\n",
      "3/3 [==============================] - 0s 48ms/step - loss: 6.0435 - val_loss: 5.3445\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 4.9701 - val_loss: 3.6457\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.9049 - val_loss: 1.0725\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.2811 - val_loss: 1.8878\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.5043 - val_loss: 0.4416\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6387 - val_loss: 1.1224\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.0314 - val_loss: 0.5248\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5070 - val_loss: 0.8190\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7549 - val_loss: 0.3578\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4002 - val_loss: 0.6472\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5564 - val_loss: 0.3170\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3886 - val_loss: 0.4183\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2947 - val_loss: 0.4029\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3625 - val_loss: 0.2839\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2647 - val_loss: 0.2653\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2418 - val_loss: 0.2641\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2228 - val_loss: 0.2736\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2412 - val_loss: 0.2683\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2141 - val_loss: 0.3163\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2552 - val_loss: 0.2955\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2335 - val_loss: 0.3008\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2580 - val_loss: 0.2560\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2183 - val_loss: 0.2541\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2240 - val_loss: 0.2289\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1937 - val_loss: 0.2158\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1889 - val_loss: 0.1927\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1742 - val_loss: 0.1925\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1675 - val_loss: 0.2153\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1839 - val_loss: 0.1819\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1792 - val_loss: 0.2108\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1968 - val_loss: 0.1947\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1702 - val_loss: 0.1652\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1527 - val_loss: 0.1545\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1371 - val_loss: 0.1626\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1367 - val_loss: 0.1397\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1313 - val_loss: 0.1279\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1376 - val_loss: 0.1723\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1519 - val_loss: 0.1736\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1381 - val_loss: 0.1391\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1391 - val_loss: 0.1688\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1659 - val_loss: 0.1898\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1447 - val_loss: 0.1792\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1390 - val_loss: 0.1246\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1147 - val_loss: 0.0945\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0987 - val_loss: 0.1364\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1114 - val_loss: 0.0856\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0933 - val_loss: 0.0961\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1115 - val_loss: 0.1605\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1479 - val_loss: 0.0921\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1193 - val_loss: 0.1599\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1463 - val_loss: 0.1577\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1477 - val_loss: 0.1031\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1280 - val_loss: 0.1584\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1526 - val_loss: 0.2058\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1578 - val_loss: 0.0877\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1321 - val_loss: 0.1216\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1485 - val_loss: 0.1647\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1624 - val_loss: 0.1195\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1412 - val_loss: 0.1524\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1923 - val_loss: 0.2324\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2318 - val_loss: 0.2597\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2969 - val_loss: 0.1646\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1840 - val_loss: 0.1980\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1992 - val_loss: 0.2227\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1803 - val_loss: 0.1525\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1335 - val_loss: 0.1633\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1647 - val_loss: 0.1169\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1810 - val_loss: 0.1308\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1924 - val_loss: 0.1163\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1816 - val_loss: 0.1376\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1874 - val_loss: 0.1140\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1353 - val_loss: 0.1084\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1190 - val_loss: 0.1606\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1341 - val_loss: 0.1569\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1485 - val_loss: 0.1425\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1565 - val_loss: 0.0878\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1630 - val_loss: 0.1224\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1492 - val_loss: 0.0706\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1528 - val_loss: 0.1344\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1422 - val_loss: 0.1301\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1428 - val_loss: 0.1173\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1004 - val_loss: 0.0769\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0794 - val_loss: 0.0741\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0748 - val_loss: 0.0682\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0835 - val_loss: 0.0958\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1069 - val_loss: 0.0823\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0899 - val_loss: 0.1086\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1032 - val_loss: 0.0664\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0756 - val_loss: 0.0590\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0757 - val_loss: 0.0680\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0860 - val_loss: 0.0998\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0891 - val_loss: 0.0669\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0845 - val_loss: 0.0939\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0934 - val_loss: 0.0581\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0660 - val_loss: 0.0540\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0690 - val_loss: 0.0590\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0658 - val_loss: 0.0926\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0935 - val_loss: 0.0828\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1052 - val_loss: 0.1650\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1381 - val_loss: 0.1673\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.1378 - val_loss: 0.0717\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0833 - val_loss: 0.0856\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0799 - val_loss: 0.0516\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0664 - val_loss: 0.0802\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0732 - val_loss: 0.0598\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0663 - val_loss: 0.0516\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0648 - val_loss: 0.0558\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0852 - val_loss: 0.0839\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1130 - val_loss: 0.0674\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0886 - val_loss: 0.1070\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1039 - val_loss: 0.0688\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0709 - val_loss: 0.0518\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0685 - val_loss: 0.0632\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0686 - val_loss: 0.1056\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1028 - val_loss: 0.0455\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1050 - val_loss: 0.1251\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1198 - val_loss: 0.1523\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1417 - val_loss: 0.0665\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0885 - val_loss: 0.1632\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1534 - val_loss: 0.1021\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0874 - val_loss: 0.0572\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0617 - val_loss: 0.1106\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0945 - val_loss: 0.0696\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0715 - val_loss: 0.0658\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0778 - val_loss: 0.0603\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0758 - val_loss: 0.0591\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0799 - val_loss: 0.0590\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0678 - val_loss: 0.0452\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0717 - val_loss: 0.0489\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0737 - val_loss: 0.0557\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0931 - val_loss: 0.1054\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1054 - val_loss: 0.0981\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1176 - val_loss: 0.0706\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1146 - val_loss: 0.1513\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1319 - val_loss: 0.1889\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1642 - val_loss: 0.1658\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1391 - val_loss: 0.0650\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0870 - val_loss: 0.0628\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0839 - val_loss: 0.0607\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0847 - val_loss: 0.0810\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0869 - val_loss: 0.0673\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0764 - val_loss: 0.0534\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0719 - val_loss: 0.0635\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1146 - val_loss: 0.1175\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1202 - val_loss: 0.1611\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1428 - val_loss: 0.1699\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1566 - val_loss: 0.1143\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1410 - val_loss: 0.1190\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1572 - val_loss: 0.0892\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1054 - val_loss: 0.1128\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0969 - val_loss: 0.0781\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0913 - val_loss: 0.1286\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1228 - val_loss: 0.1103\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1310 - val_loss: 0.1033\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1464 - val_loss: 0.0984\n",
      "Epoch 156/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1032 - val_loss: 0.1021\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0881 - val_loss: 0.0780\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0715 - val_loss: 0.0570\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0755 - val_loss: 0.1266\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1137 - val_loss: 0.0669\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1418 - val_loss: 0.0630\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1465 - val_loss: 0.1441\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1653 - val_loss: 0.0945\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1156 - val_loss: 0.0858\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1032 - val_loss: 0.0840\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1122 - val_loss: 0.0607\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0830 - val_loss: 0.0836\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0793 - val_loss: 0.0635\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0822 - val_loss: 0.0742\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0784 - val_loss: 0.1074\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1019 - val_loss: 0.0949\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0986 - val_loss: 0.0750\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0795 - val_loss: 0.0882\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best Model's Parameters\n",
      "{'learning_rate': 0.001, 'n_hidden': 3, 'n_neurons': 600, 'build_fn': <function build_model at 0x150040c10>}\n",
      "\n",
      " The mean absolute error (MAE)\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.0882\n",
      "-0.088227778673172\n",
      "\n",
      " The predicted values (Lattice constants)\n",
      "[[5.8470426 8.007292  5.7135477]\n",
      " [5.4070687 7.2935596 5.162371 ]\n",
      " [5.669082  7.6204467 5.38502  ]\n",
      " [5.6049433 7.584451  5.3782306]\n",
      " [5.578819  7.415848  5.208065 ]\n",
      " [5.6410336 7.4401603 5.2019415]\n",
      " [5.6767454 7.6751122 5.4404464]\n",
      " [5.608989  7.611943  5.4059434]\n",
      " [5.6491675 7.494474  5.2580523]\n",
      " [5.6028843 7.570637  5.364315 ]\n",
      " [5.6467776 7.480515  5.243821 ]\n",
      " [5.5722876 7.3740554 5.165058 ]\n",
      " [5.6516185 7.508602  5.2722583]\n",
      " [5.9629583 7.4268165 4.945278 ]\n",
      " [6.011265  8.168544  5.806748 ]\n",
      " [5.6516185 7.508602  5.2722583]\n",
      " [5.669082  7.6204467 5.38502  ]\n",
      " [6.2238383 8.558459  6.1192093]\n",
      " [5.460732  7.391255  5.2412534]\n",
      " [5.7225366 7.7180047 5.4639726]]\n"
     ]
    }
   ],
   "source": [
    "# GridSearch for the ANN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_distribs = {\n",
    "#     \"n_hidden\": [1, 2, 3, 4],\n",
    "#     \"n_neurons\": [900, 1100, 1300, 1500],\n",
    "#     \"learning_rate\": [1e-2, 1e-3],\n",
    "# }\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [1, 2, 3, 4],\n",
    "    \"n_neurons\": [300, 400, 500, 600],\n",
    "    \"learning_rate\": [1e-3, 1e-4],\n",
    "}\n",
    "\n",
    "search_cv = GridSearchCV(estimator=keras_reg,\n",
    "                         param_grid=param_distribs, \n",
    "                         cv=4, \n",
    "#                          scoring ='neg_mean_absolute_error',\n",
    "                        #  refit=False,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "\n",
    "# Best estimator is selected for further proceedings\n",
    "grid_result = search_cv.fit(X_train, y_train,\n",
    "                  validation_data=(X_test, y_test),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"mean: %f, std: (%f) parameters: %r\" % (mean, stdev, param))\n",
    "\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "print (\"\\n History objects\")\n",
    "best_model_history = best_model.fit(X_train, y_train, epochs=n_epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=tolerance_value)])\n",
    "              \n",
    "plt.figure()\n",
    "plt.plot(best_model_history.history['loss'])\n",
    "plt.plot(best_model_history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss (MAE)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.show()\n",
    "\n",
    "print (\"\\n Best Model's Parameters\")\n",
    "print (best_model.get_params())\n",
    "print (\"\\n The mean absolute error (MAE)\")\n",
    "print (best_model.score(X_test, y_test))\n",
    "print (\"\\n The predicted values (Lattice constants)\")\n",
    "print (best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-alarm",
   "metadata": {
    "id": "PyWxHUQn92zG"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The prediction of lattice constants for $ABO_{3}$ from the atomic configurations like radii and tolerance has enigmatic mechanism. Which is why neural network is the best fit for thr job. ANN has been elusive when researchers tried to uncover the it's intricate function.\n",
    "\n",
    "In this notebook too I deployed the ANN and found was successful in predicting the lattice constants with the mean absolute error around -0.055. Which is 5.5% error. The GridSearch did not perform well I would have expected it do. Howerver, it along a few techniques like feature scaling and the commented out linear model is next task in my schedule. \n",
    "\n",
    "This notebook could not be amongst the best performer. However, especially after building an ANN I am motivated to work further on Machine Learning domain. I have all the incentives to move ahead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-canal",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Chonghe, Li, et al. \"Prediction of lattice constant in perovskites of GdFeO3 structure.\" Journal of Physics and Chemistry of Solids 64.11 (2003): 2147-2156.\n",
    "[2] Alade, Ibrahim Olanrewaju, Ismail Adewale Olumegbon, and Aliyu Bagudu. \"Lattice constant prediction of A2XY6 cubic crystals (A= K, Cs, Rb, TI; X= tetravalent cation; Y= F, Cl, Br, I) using computational intelligence approach.\" Journal of Applied Physics 127.1 (2020): 015303.\n",
    "[3] Jiang, L. Q., et al. \"Prediction of lattice constant in cubic perovskites.\" Journal of Physics and Chemistry of Solids 67.7 (2006): 1531-1536.\n",
    "[4] “Introduction to Machine Learning with Python, A Guide for Data Scientists\", by Andreas C. Müller and Sarah Guido\n",
    "[5] “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\", by Aurélien Géron\n",
    "[6] \"The Hundred-Page Machine Learning Book\", by Andriy Burkov"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ANN-ABO3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
